[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m Carlos Hinojosa. Computer scientist and engineer with over six years of experience in scientific research and software development. I received the B. Sc. and M. Sc. degrees in Computer Science from Universidad Industrial de Santander, Colombia, in 2015 and 2018, respectively. Currently, I am pursuing a Ph.D. in Computer Science at Universidad Industrial de Santander. My research interests are in computer vision, machine learning, computational imaging, sparse representation, and signal processing. I also have developed scientific software for companies like Ecopetrol S.A and my university under distinct research projects. I have experience with different programming languages, including C/C++, Java, C#, Javascript, MATLAB, Typescript, Python, and Bash scripting in Linux-like operating systems. Furthermore, I have experience with Linux system administration and hybrid mobile app development with different frameworks like Ionic, React, and Cordova.\n","date":1663100115,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1663100115,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"http://carloshinojosa.me/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I\u0026rsquo;m Carlos Hinojosa. Computer scientist and engineer with over six years of experience in scientific research and software development. I received the B. Sc. and M. Sc. degrees in Computer Science from Universidad Industrial de Santander, Colombia, in 2015 and 2018, respectively. Currently, I am pursuing a Ph.D. in Computer Science at Universidad Industrial de Santander. My research interests are in computer vision, machine learning, computational imaging, sparse representation, and signal processing.","tags":null,"title":"Carlos Hinojosa","type":"authors"},{"authors":null,"categories":null,"content":"Introducción El objetivo de este laboratorio es brindar al estudiante HDSP las herramientas base para el desarrollo de algoritmos de aprendizaje profundo (Deep Learning en inglés) que le permitan solucionar distintas tareas de visión por computadora como detección y clasificación de objetos, así como tambien resolver problemas inversos como la recuperación o reconstrucción de imágenes espectrales comprimidas. El laboratorio estará dividio en x secciones donde el estudiante deberá resolver y/o ejecutar los respectivos ejercicios y responder preguntas o analizar los resultados obtenidos al final de cada sección.\nDesarrollo de los Laboratorios Todos los laboratorios de Deep Learning serán entregados al estudiante HDSP en formato notebook de Python a través de la plataforma de Google Colaboratory. La principal razón para realizar los laboratorios en este formato es que Google Colaboratory, o simplemente Colab, permite ejecutar y programar en Python directamente desde el navegador. De esta manera se facilita la configuración de un entorno para Deep Learning y el accesso gratuito a GPUs para ejecutar los algoritmos desarrollados.\nAntes de iniciar cada laboratorio, el estudiante debe dar click en el boton lo cual abrirá el notebook, correspondiente al laboratorio, directamente en Colab. Luego, para poder editar y ejecutar el código, el estudiante debe copiar el notebook en su Google Drive. Para esto, debe tener abierto el notebook en Colab y luego dar click en el boton Una vez completado el anterior procedimiento, el estudiante debe seguir la guía proporcionada donde se abordarán temas desde un nivel básico hasta un nivel intermedio. Al final de cada guía el estudiante deberá responder preguntas y/o resolver ejercicios. Un vez completado los ejercicios, el estudiante deberá compartir el notebook a su tutor y preparar un documento donde se resuma lo aprendido y se analicen los resultados obtenidos.\n","date":1591056000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1591056000,"objectID":"959986049602592b8f4a30dbe2289b2f","permalink":"http://carloshinojosa.me/courses/deep-learning-labs/","publishdate":"2020-06-02T00:00:00Z","relpermalink":"/courses/deep-learning-labs/","section":"courses","summary":"Laboratorios de Deep Learning correspondientes al periodo de prueba para ingresar al grupo HDSP de la Universidad Industrial de Santander.","tags":null,"title":"HDSP - Laboratorio de Deep Learning","type":"docs"},{"authors":null,"categories":null,"content":"\nAntes de empezar, da click en el boton \u0026ldquo;Open in Colab\u0026rdquo; de arriba. Esto abrirá este notebook de python directamente en Colab. Luego, para poder editar y ejecutar el código, deberas copiar el notebook en Google drive utilizando el boton:\nDe esta manera, este notebook quedará almacenado en Google drive, en la carpeta Colab Notebooks\nIntroducción El objetivo de este tutorial es proporcionar un flujo de trabajo para entrenar modelos de Deep Learning. El flujo de trabajo propuesto requiere tener instalado los siguientes componentes en nuestra computadora:\nCliente de sincronización de Google Drive. Editor de texto o IDE de python como Pycharm. La idea general será tener nuestro código python en una carpeta en Google Drive. De esta manera, utilizaremos el cliente de sincronización para modificar los archivos de manera local en nuestras computadoras (utilizando el editor de texto o un IDE de python) y que los cambios se vean reflejados directamente en Colab. Aunque Colab permite editar archivos (simplemente dando doble click en el archivo deseado en el menu de la izquierda) es mucho mas sencillo y cómodo editar y manipular archivos complejos en un IDE de python.\nCliente Google Drive Para sistemas operativos Windows y Mac el cliente de Google drive puede ser descargado directamente de la pagina de google drive\nhttps://www.google.com/drive/download/\nSin embargo, para sistemas operativos Linux no existe un cliente oficial de google drive, pero existen alternativas excelentes como Insync (Software pago una única vez) o el cliente para sistemas Ubuntu:\nhttps://cambiatealinux.com/instalar-google-drive-en-ubuntu\nEditor de Python Para trabajar con proyectos Python, es recomendable utilizar el entorno de desarrollo integrado (IDE) Pycharm. Actualmente, es posible acceder a una licencia de estudiante de la version profesional de Pycharm por un año:\nhttps://www.jetbrains.com/es-es/community/education/#students\nPara acceder al beneficio solo se necesita el correo electronico institucional (@correo.uis.edu.co o @saber.uis.edu.co o @uis.edu.co).\nSi no se desea utilizar un IDE, es posible trabajar con un editor avanzado de texto: Visual Studio Code:\nhttps://code.visualstudio.com/\nNote que, debido a que nuestra intención es solo modificar el código en nuestra computadora local y ejecutar el código en Colab, no necesitamos tener instalado Python y las librerias de Tensorflow en nuestra computadora ya que estas se encuentran instaladas en Colab. Sin embargo, es recomendable instalar Python (Anaconda) y configurarlo con Pycharm en nuestra computadora local. A continuación proporciono links de interes:\nInstalar Anaconda: Windows: https://docs.anaconda.com/anaconda/install/windows/, Linux: https://docs.anaconda.com/anaconda/install/linux/, Mac: https://docs.anaconda.com/anaconda/install/mac-os/\nConfigurar Pycharm con Anaconda: https://docs.anaconda.com/anaconda/user-guide/tasks/pycharm/\nInstalar liberias de Deep Learning: https://asociacionaepi.es/primeros-pasos-con-tensorflow/\nSolicitar Recursos a Colab Una vez creemos un notebook en Colab, para solicitar recursos debemos primero seleccionar el entorno de ejecución adecuado y dar click en el botón Conectar:\nUna vez conectados, Colab nos asigna un entorno Linux con 25.51 GB de Ram, 68 GB de disco duro y una GPU cuyas caracteristicas podemos consultar ejecutando la siguiente celda\nCaracterísticas de la GPU asignada # Check nvidia and nvcc cuda compiler !nvidia-smi !/usr/local/cuda/bin/nvcc --version Tue May 19 00:16:53 2020 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.82 Driver Version: 418.67 CUDA Version: 10.1 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 | | N/A 34C P0 26W / 250W | 0MiB / 16280MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2019 NVIDIA Corporation Built on Sun_Jul_28_19:07:16_PDT_2019 Cuda compilation tools, release 10.1, V10.1.243 Por lo general, Colab asigna de manera aleatoria GPUs con diferente cantidad de memoria. Es recomendable utilizar una GPU de 16 o 15 GB de memoria para proyectos que requieran mucha memoria o trabajen con muchos datos. En caso de que Colab no nos asigne una GPU adecuada (algunas veces asigna GPUs de 8 o 11 GB de memoria) para nuestro proyecto, siempre podemos realizar distintas solicitudes hasta conseguir una GPU adecuada. Para esto debemos cerrar la sesion actual y volver a solicitar recursos:\nEste proceso se repite varias veces hasta que se nos asigne la GPU deseada. Note que es posible que requiera recargar la ventana (F5) para poder finalizar la sesion. OJO Esto solo es necesario hacerlo si necesitamos una GPU de un tamaño de memoria especifico. Si nuestro proyecto es pequeño, cualquier GPU asignada por Colab servirá.\nNota: Este notebook está pre-configurado para trabajar con 25 Gb de memoria RAM, sin embargo esta configuración no es por defecto. La cantidad de memoria asignada por general es de 12 a 16 GB. Si crea un notebook nuevo en Colab y quiere tener 25 GB ver sección de Bonus al final de este tutorial.\nComandos Básicos Debido a que cada notebook de Colab se ejecuta en una máquina Linux, es posible utilizar todos los comandos de Linux. Los comandos más básicos para movernos dentro de este tipo de ambiente son:\n%pwd # Ver el directorio actual de trabajo %ls # Listar los archivos del directorio actual %cd # Cambiar de directorio %mkdir # Crear un nuevo directorio %rmdir # Eliminar un directorio vacio Por ejemplo, si quiero ver en que carpeta me encuentro actualmente ejecuto una celda con el comando:\n%pwd '/content' Como vemos, me encuentro en la carpeta \u0026lsquo;/content\u0026rsquo;, es decir, la carpeta raiz de Colab. Adicionalmente, podemos ejecutar varios comandos dentro de una celda agregando %%shell al inicio de esta. Por ejemplo:\n%%shell pwd ls mkdir \u0026#34;nuevo_directorio\u0026#34; ls rmdir \u0026#34;nuevo_directorio\u0026#34; ls /content sample_data nuevo_directorio sample_data sample_data Note que si se agrega %%shell al inicio, se debe quitar el simbolo % al principio de cada comando. Así, en vez de escribir %pwd, escribimos pwd solamente. Note ademas, que el resultado de cada comando se muestra en una linea aparte. De esta manera, el primer comando ls, muestra el contenido de la carpeta /content, que en este caso es solo la carpeta \u0026lsquo;sample_data/\u0026rsquo;. Luego creamos la carpeta \u0026rsquo;nuevo_directorio\u0026rsquo; (observe que el comando mkdir no arroja ninguna salida), listamos el contenido de content/ para ver la nueva carpeta creada y por ultimo borramos la carpeta.\nEn general se puede utilizar cualquier comando linux, incluso instalar paquetes linux con el software apt, por ejemplo:\n%%shell sudo apt install nano %%shell sudo apt install nano Reading package lists... Done Building dependency tree Reading state information... Done Suggested packages: spell The following NEW packages will be installed: nano 0 upgraded, 1 newly installed, 0 to remove and 31 not upgraded. Need to get 231 kB of archives. After this operation, 778 kB of additional disk space will be used. Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 nano amd64 2.9.3-2 [231 kB] Fetched 231 kB in 2s (130 kB/s) debconf: unable to initialize frontend: Dialog debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, \u0026lt;\u0026gt; line 1.) debconf: falling back to frontend: Readline debconf: unable to initialize frontend: Readline debconf: (This frontend requires a controlling tty.) debconf: falling back to frontend: Teletype dpkg-preconfigure: unable to re-open stdin: Selecting previously unselected package nano. (Reading database ... 144433 files and directories currently installed.) Preparing to unpack .../nano_2.9.3-2_amd64.deb ... Unpacking nano (2.9.3-2) ... Setting up nano (2.9.3-2) ... update-alternatives: using /bin/nano to provide /usr/bin/editor (editor) in auto mode update-alternatives: using /bin/nano to provide /usr/bin/pico (pico) in auto mode Processing triggers for man-db (2.8.3-2ubuntu0.1) ... Puede buscar mas comandos en internet, una pagina inicial sería:\nhttps://marcosmarti.org/comandos-basicos-de-linux/\nSetup Lo primero que haremos será conectarnos con nuestra cuenta de Google Drive. Importante conectarse a la cuenta de google drive en donde se tiene almacenado el proyecto o el código a ejecutar. Adicionalmente, esta cuenta debe estar previamente sincronizada en nuestra computadora utilizando un cliente de Google Drive (Ver Introducción). Para conectarnos a google drive, ejecutamos la celda de abajo y seguimos los pasos. Por favor seleccione la cuenta previamente sincronizada.\nMount Goolge Drive # link to google drive from google.colab import drive #drive.mount(\u0026#39;/content/gdrive/\u0026#39;) drive.mount(\u0026#34;/content/gdrive/\u0026#34;, force_remount=True) Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com\u0026amp;redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob\u0026amp;response_type=code\u0026amp;scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly Enter your authorization code: ·········· Mounted at /content/gdrive/ Si damos click en el panel de la izquierda, al icono de carpeta, veremos que tenemos una nueva carpeta llamada \u0026lsquo;gdrive\u0026rsquo;. De esta manera habremos sincronizado de manera correcta google drive con Colab.\nAhora la idea es trabajar en nuestro código. Existen básicamente dos maneras:\nAñadir el codigo directamente en este notebook e ir ejecutando cada celda. Ejecutar un script de python directamente desde una celda. En este tutorial se utilizará la segunda forma. Para esto utilicé como ejemplo el tutorial de tensorflow:\nhttps://www.tensorflow.org/tensorboard/get_started\nSeguidamente, creé un proyecto en mi computadora local llamado colab_tutorial. La carpeta de este proyecto se encuentra sincronizado con mi Google Drive, por lo tanto, todo cambio que se realice en mi computadora local se vera reflejado directamente en Drive y, de igual forma, en Colab.\nUna vez tengamos listo el código en nuestra carpeta local, solo basta con ejecutar una celda de la siguiente forma\n!python3 nombre_archivo.py --parametros (opcional) De esta forma ejecutaremos nuestro código en Colab. Note que nuestro código tambien puede generar salidas, las cuales (si se almacenan dentro de la misma carpeta del proyecto) quedarán sincronizadas con nuestra computadora local.\nAqui un ejemplo de ejecutar la funcion main.py dentro de mi carpeta proyecto \u0026ldquo;colab_tutorial\u0026rdquo;. Primero vamos hacia la carpeta con los comandos básicos explicados en la sección 2.\n%cd /content/gdrive/My\\ Drive/colab_tutorial/ /content/gdrive/My Drive/colab_tutorial %ls main.py !python main.py 2020-05-19 01:24:56.751869: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1 2020-05-19 01:24:58.852417: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1 2020-05-19 01:24:58.866042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-19 01:24:58.866658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0 coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s 2020-05-19 01:24:58.866696: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1 2020-05-19 01:24:58.868230: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10 2020-05-19 01:24:58.869913: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10 2020-05-19 01:24:58.870253: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10 2020-05-19 01:24:58.871814: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10 2020-05-19 01:24:58.872594: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10 2020-05-19 01:24:58.875551: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7 2020-05-19 01:24:58.875654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-19 01:24:58.876241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-19 01:24:58.876770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0 2020-05-19 01:24:58.881933: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200000000 Hz 2020-05-19 01:24:58.882377: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x138f100 initialized for platform Host (this does not guarantee that XLA will be used). Devices: 2020-05-19 01:24:58.882409: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version 2020-05-19 01:24:58.960753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-19 01:24:58.961607: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x138ef40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices: 2020-05-19 01:24:58.961643: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0 2020-05-19 01:24:58.961839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-19 01:24:58.962429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0 coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s 2020-05-19 01:24:58.962471: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1 2020-05-19 01:24:58.962526: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10 2020-05-19 01:24:58.962541: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10 2020-05-19 01:24:58.962554: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10 2020-05-19 01:24:58.962566: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10 2020-05-19 01:24:58.962581: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10 2020-05-19 01:24:58.962596: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7 2020-05-19 01:24:58.962655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-19 01:24:58.963213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-19 01:24:58.963746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0 2020-05-19 01:24:58.963791: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1 2020-05-19 01:24:59.498013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix: 2020-05-19 01:24:59.498074: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108] 0 2020-05-19 01:24:59.498084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0: N 2020-05-19 01:24:59.498276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-19 01:24:59.498864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-19 01:24:59.499412: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0. 2020-05-19 01:24:59.499466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14973 MB memory) -\u0026gt; physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0) 2020-05-19 01:24:59.547390: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session started. 2020-05-19 01:24:59.547453: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1363] Profiler found 1 GPUs 2020-05-19 01:24:59.548374: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcupti.so.10.1 2020-05-19 01:24:59.678606: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1479] CUPTI activity buffer flushed Epoch 1/10 2020-05-19 01:25:00.110434: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10 2020-05-19 01:25:00.407164: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session started. 1/1875 [..............................] - ETA: 0s - loss: 2.5025 - accuracy: 0.0000e+002020-05-19 01:25:00.411242: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1479] CUPTI activity buffer flushed 2020-05-19 01:25:00.411386: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:216] GpuTracer has collected 62 callback api events and 62 activity events. 2020-05-19 01:25:00.424149: I tensorflow/core/profiler/rpc/client/save_profile.cc:168] Creating directory: logs/fit/20200519-012459/train/plugins/profile/2020_05_19_01_25_00 2020-05-19 01:25:00.430074: I tensorflow/core/profiler/rpc/client/save_profile.cc:174] Dumped gzipped tool data for trace.json.gz to logs/fit/20200519-012459/train/plugins/profile/2020_05_19_01_25_00/998f21424347.trace.json.gz 2020-05-19 01:25:00.430893: I tensorflow/core/profiler/utils/event_span.cc:288] Generation of step-events took 0.016 ms 2020-05-19 01:25:00.449145: I tensorflow/python/profiler/internal/profiler_wrapper.cc:87] Creating directory: logs/fit/20200519-012459/train/plugins/profile/2020_05_19_01_25_00Dumped tool data for overview_page.pb to logs/fit/20200519-012459/train/plugins/profile/2020_05_19_01_25_00/998f21424347.overview_page.pb Dumped tool data for input_pipeline.pb to logs/fit/20200519-012459/train/plugins/profile/2020_05_19_01_25_00/998f21424347.input_pipeline.pb Dumped tool data for tensorflow_stats.pb to logs/fit/20200519-012459/train/plugins/profile/2020_05_19_01_25_00/998f21424347.tensorflow_stats.pb Dumped tool data for kernel_stats.pb to logs/fit/20200519-012459/train/plugins/profile/2020_05_19_01_25_00/998f21424347.kernel_stats.pb 1875/1875 [==============================] - 5s 3ms/step - loss: 0.2168 - accuracy: 0.9365 - val_loss: 0.1001 - val_accuracy: 0.9694 Epoch 2/10 1875/1875 [==============================] - 5s 3ms/step - loss: 0.0948 - accuracy: 0.9710 - val_loss: 0.0865 - val_accuracy: 0.9725 Epoch 3/10 1875/1875 [==============================] - 5s 3ms/step - loss: 0.0670 - accuracy: 0.9786 - val_loss: 0.0708 - val_accuracy: 0.9778 Epoch 4/10 1875/1875 [==============================] - 5s 2ms/step - loss: 0.0525 - accuracy: 0.9832 - val_loss: 0.0623 - val_accuracy: 0.9810 Epoch 5/10 1875/1875 [==============================] - 5s 2ms/step - loss: 0.0433 - accuracy: 0.9855 - val_loss: 0.0696 - val_accuracy: 0.9806 Epoch 6/10 1875/1875 [==============================] - 5s 2ms/step - loss: 0.0357 - accuracy: 0.9885 - val_loss: 0.0685 - val_accuracy: 0.9816 Epoch 7/10 1875/1875 [==============================] - 5s 2ms/step - loss: 0.0299 - accuracy: 0.9901 - val_loss: 0.0625 - val_accuracy: 0.9822 Epoch 8/10 1875/1875 [==============================] - 5s 2ms/step - loss: 0.0276 - accuracy: 0.9908 - val_loss: 0.0655 - val_accuracy: 0.9821 Epoch 9/10 1875/1875 [==============================] - 5s 2ms/step - loss: 0.0237 - accuracy: 0.9925 - val_loss: 0.0789 - val_accuracy: 0.9806 Epoch 10/10 1875/1875 [==============================] - 5s 2ms/step - loss: 0.0230 - accuracy: 0.9923 - val_loss: 0.0750 - val_accuracy: 0.9827 En este ejemplo en particular se hizo uso de la herramienta Tensorboard, el cual proporciona una interfaz de visualización para el entrenamiento de los modelos. Al ejecutar nuestro código, se guarda en la carpeta logs el resultado de entrenamiento para cada epoch. Para visualizar la herramienta Tensorboard tenemos basicamente dos formas:\nVisualizar Tensorboard directamente en Colab Visualizar Tensorboard en nuestra computadora local La desventaja de visualizar en Colab es que debemos esperar a que termine el entrenamiento para visualizar Tensorboard. Por el contrario, debido a que tenemos nuestra carpeta sincronizada, podemos ejecutar Tensorboard en nuestra computadora y visualizar el entrenamiento de manera local en tiempo real mientras se ejecuta el entrenamiento en Colab.\nPara visualizar el resultado directamente en este notebook ejecutamos las siguientes dos celdas. La primera solo se ejecuta una vez, pues es la encargada de cargar la extension.\n# Load the TensorBoard notebook extension (Ejecutar una sola vez) %load_ext tensorboard %tensorboard --logdir logs/fit Reusing TensorBoard on port 6006 (pid 1921), started 0:04:55 ago. (Use '!kill 1921' to kill it.) \u0026lt;IPython.core.display.Javascript object\u0026gt; Los detalles de implementación del código utilizado de ejemplo se pueden consultar directamente en la página del tutorial: https://www.tensorflow.org/tensorboard/get_started\nConclusiones Conclusion En conclusion, gracias a la sincronización que nos provee el cliente de google drive, podemos hacer todas las modificaciones necesarias de manera local en nuestras computadoras y unicamente utilizar colab para ejecturar el entrenamiento de la red neuronal aprovechando su GPU. Adicionalmente, el uso de la herramienta Tensorboard es fundamental para monitorear el entrenamiento.\nBonus 1) Cuando ejecutamos entrenamientos muy largos, es posible que Colab se desconecte inesperadamente debido a falta de interactividad. Para solucionar este problema podemos agregar un código Javascript en esta página para hacer \u0026ldquo;clicks\u0026rdquo; de manera automática en el boton Conect (Boton donde se muestra uso de RAM y Disco). El código en cuestión es el siguiente:\nfunction ClickConnect(){ console.log(\u0026#34;Clicked on connect button\u0026#34;); document.querySelector(\u0026#34;colab-connect-button\u0026#34;).click() } setInterval(ClickConnect,60000) Este código debe agregarse en la consola de Javascript de la siguiente manera:\n2) Si se quiere mas memoria ram, se puede ejecutar el siguiente código en una celda:\na = [] while(1): a.append(‘1’) Al ejecutarlo, este comando va a llenar la memoria ram disponible en Colab y forzará a este a ampliar la capacidad. Debemos esperar aproximadamente 1 minuto hasta que salga un letrero que pregunta si deseamos ampliar la memoria. Despues de aceptar, tendremos más memoria RAM en nuestro notebook. Mas información: https://towardsdatascience.com/upgrade-your-memory-on-google-colab-for-free-1b8b18e8791d\nTarea Realice las siguientes actividades y elabore un informe con sus análisis y conclusiones\nLea el tutorial descrito en el siguiente enlace: https://www.tensorflow.org/tensorboard/get_started Instale el IDE de python Pycharm y el cliente de Google drive como se describe en este tutorial. Implemente el código mostrado en el tutorial https://www.tensorflow.org/tensorboard/get_started en un archivo de python (.py). Sincronice el código con Google Drive utilizando el cliente de Google drive Ejecute el código en Colab y visualice la salida en Tensorboard. Utilice el editor Pycharm para modificar el archivo de python donde se implementó el código. Cambie el número de epocas (EPOCHS) a 10 y vuelva a ejecutar el código. Elabore un informe corto donde describa con sus palabras que es Google Colab y que es Tensorboard y para qué sirve estas dos herramientas. ","date":1593385200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593385200,"objectID":"40f68b8fdcd051bdb40e30242506b36e","permalink":"http://carloshinojosa.me/courses/deep-learning-labs/colab/","publishdate":"2020-06-29T00:00:00+01:00","relpermalink":"/courses/deep-learning-labs/colab/","section":"courses","summary":"Antes de empezar, da click en el boton \u0026ldquo;Open in Colab\u0026rdquo; de arriba. Esto abrirá este notebook de python directamente en Colab. Luego, para poder editar y ejecutar el código, deberas copiar el notebook en Google drive utilizando el boton:\nDe esta manera, este notebook quedará almacenado en Google drive, en la carpeta Colab Notebooks\nIntroducción El objetivo de este tutorial es proporcionar un flujo de trabajo para entrenar modelos de Deep Learning.","tags":null,"title":"Google Colaboratory","type":"docs"},{"authors":null,"categories":null,"content":"Antes de inicar con conceptos básicos sobre redes neuronales tensorflow keras etc, estudiaremos las ideas fundamentales sobre el mundo de la inteligencia artificial.\nLa inteligencia artificial se puede definir como la ciencia de entrenar máquinas para ejecutar tareas humanas y machine learning es una parte de la inteligencia artifical encargada de resolver esos problemas.\nImage(\u0026#39;ML.PNG\u0026#39;, width = 800) Machine Learning es un metodo de análisis para la construcción de modelos automatas, que usan diversos algoritmos para aprender iterativamente de un conjunto de datos historicos.\nLos campos de aplicación de machine learning son muy diversos:\nDetección de fraudes Resultados de motores de busqueda Análisis de sentimientos Detección de objetos Reconocimiento de voz Realce de imágenes. Uno de los sistemas de machine learning que ha revolucionado la industria en los ultimos años son las conocidas redes neuronales.\nLas redes neuronales son una forma de modelar matematicamente un neuron biológico. Esto es, son arquitecturas inspiradas en el cerebro humano y en su estructura ampliamente paralela, con capacidad de adquirir, almacenar y utilizar conocimiento experimental, generalizando su conocimiento a partir de ejemplos anteriores.\nEn 1943 se lanza el primer modelo computacional para una neurona artificial y es llamado de perceptron.\nEl perceptron (Figura 1), es la idea mas simple de un neuron artificial y se puede definir como el conjunto de entradas de una muestra X las cuales representan los estimulos que la neurona recibe del ambiente y que se combinan con los pesos sinápticos W para determinar la entrada del neuron, dicha entrada pasa por una función de activación o umbral que genera una salida Y la cual determinará si el neuron se activa siempre y cuando el valor de la combinacion lineal de sus entradas con los pesos supere el umbral de activación.\nImage(\u0026#39;perceptron.PNG\u0026#39;, width = 800) En un neuron podemos codificar información que nos servira para realizar tareas de calsificación prediccion, etc. Sin embargo la figura de un perceptron matematicamente (antes de la funcion de activación), no es mas que un función que realiza un calculo numérico lineal. Internamente la neurona utiliza todos los valores de entrada para realizar una suma ponderada entre los estimulos X multiplicados por un valor especifico llamado pesos W y que son los parametros y la parte fundamental del proceso de aprendizaje.\n$$Y = x_1w_1 + x_2w_2 + x_3w_3 ..+ ..+ ..+ x_nw_n + b$$\ndonde b es el termino independiente de la ecuación y es conocido como bias el cual tomara el control para mover la recta de la función, y es una conección mas a la entrada de cada neurona.\nSi lo ven el modelo matemático de una neurona no es mas que una funcion lineal\n$$ y = aX +b$$\nUna forma práctica de analizar una neurona artificial es simulando una compuerta and donde tenemos dos posibles valores de entrada x1 , x2 y cuatro posibles combinaciones de salida y que las podemos etiquetar como 0(inactiva) y 1(activa), este nuevo concepto etiqueta es clave en el aprendizaje de máquinas supervisado ya que la neurona aprenderá de los datos históricos o también conocidos como etiquetas ( como se puede ver las etiquetas son los mismos valores de salida).\nImage(\u0026#39;tabla.PNG\u0026#39;, width = 800) Pero como se sabe el resultado de usar una regresión lineal entre las combinaciones entre x1 y x2 nos da un resultado numérico continuo y no un número binario, para eso se introducen las llamadas funciones de activación, que en la figura 1 son representadas por la ultima caja del modelo y que utilizaran un umbral para decidir si el valor numérico entregado por la suma ponderada supera dicho umbral se entregará como salida un 1 ( neurona activa) caso contrario será entregado un 0 ( neurona no activa).\nuna forma mas simple e intuitiva de ver este problema es tratandolo como un problema de clasificación. Esto es encontrar una recta que separe los valores 0 de los valores 1 como se muestra en la siguiente grafica de una compuerta and.\nImage(\u0026#39;and.PNG\u0026#39;, width = 500) La linea azul es la linea de regresión que la neurona aprendio despues de alguna etapas de entrenamiento y consiguio encontrar los mejores valores de W para lograr su objetivo ( separar los valores 0 de los valores 1).\nPero que es el entrenamiento? el entrenamiento en modelos de machine learning es la aplicacion de un algoritmo de aprendizje através de iteraciones sobre el conjunto de parametros W y b con el fin de actualizarlos en cada iteración reduciendo el error y ahciendo que la salida sea lo mas parecida posible a la etiqueta.\nEstos algoritmos de aprendizaje buscan imitar la la forma en que funcionan las neuronas en el cerebro (si generan pulsos electricos que se trasmitan a otras neuronas o no), se utiliza la formula de aprendizaje que se resume en 2 pasos:\ninicialización de pesos con valores pequeños Para cada muestra de entrada de X se cacula su respectiva salida Y y se actualizan los pesos W de la siguiente manera: $$W_j = W_j + \\Delta W_j$$\ndonde $\\Delta W_j$ es:\n$$\\Delta W_j = \\eta (Y - \\hat {Y}) X_j$$\ndonde $Y$ es el valor de la etiqueta original y $\\hat {Y}$ es el valor calculado a la salida del perceptron, usando como funcion de activación el escalon unitario.\n#Porque se deben entrenar los modelos de aprendizaje automático?\nA fin de que nuestros algoritmos de aprendizaje automático funcionen de la manera mas adecuada posible (esto es que se ajusten lo mejor posible al problema planteado), se debe llevar a cabo un mapa conceptual de que es lo que se pretende lograr con el modelo y como podemos conseguir que dicho modelo se ajuste a los datos presentados. Esto requiere de mucha practica ya que cada algoritmo lleva caonsigo sus propias caracteristicas y se basa en determinadas suposiciones, \u0026ldquo;Ningun algoritmo es perfecto todos los modelos estan equivocados pero algunos son utiles. George Eward Pelham \u0026quot;\nA dia de hoy existen, decenas de frameworks y APIs que nos facilitan crear modelos de aprendizaje automático de una forma simple y eficiente. Pero que hay detras de estos modelos? porque se habla de algoritmos y reglas de aprendizaje? y para que sirve esto?. Pues en este apartado encontraras una simple explicación del porque es importante entrenar los modelos de aprendizaje a través de un ejemplo de regresión lineal.\nTodo parte de un conjunto de datos historicos, el machine learning no existiria si no tuvieramos datos con que entrenar nuestros modelos (sem dados no há para isso ==\u0026gt; pequeña muestra de un error en un sistema de reconocimiento en portugues cuando pronuncié sin datos no hay paraiso :-)).\nComo el objetivo es crear un sistema de regresión linela, recurriremos a su formula:\n$$ y=WX+b$$\ndonde W y b como ya sabemos representan los pesos y bias como son llamados en machine learning o como matematicamente se les conoce la pendiente e intersección de la recta respectivamente, X representa el conjunto de entrada de nuestros sistemas (nuestros datos historicos) y y será el resultado deseado.\nIniciaremos creando un conjunto de características aleatorias sinteticas que representaran el conjunto de datos de entrada X, suponiendo que se trata de una variable de precio de casas en un barrio por ejemplo.\n#importamos las librerias necesarias import tensorflow as tf # generamos valores aleatorios de entrada en fomra de tensores Numero_muestras=1000 X = tf.random.normal(shape=(Numero_muestras,)) Ahora crearemos el modelo de regresión lineal, basado en la formula anterior e inicializando los pesos y el bias con valores aleatorios, esto con el fin de mostrar por que es imoprtante el entrenamiento. Atento que mencione \u0026ldquo;\u0026ldquo;inicializar\u0026rdquo;\u0026rdquo;, en todo algoritmo de machine learning en su etapa inicial los pesos y bias se inician con valores aleatorios my pequeños preferiblemente diferentes de zero. En este caso a modo de ejemplo asignaremos unos valores constantes de: W=16.0 y b=10.0\nclass Regresion(object): def __init__(self): self.W = tf.Variable(16.0) # pesos w inicializado en 16 self.b = tf.Variable(10.0) # bias inicializado en 10 def __call__(self,X): return self.W*X + self.b # ecuación de la regresión y como por arte de magia el modelo de regresión lineal esta creado de forma muy simple, donde al llamar el metodo call retornaremos nuestra función de regresión previamente explicada.\nAhora se creará una instancia del modelo y se hará una predicción\nmodelo = Regresion() # instancia del modelo modelo(20).numpy() # en este caso X es la entrada de la redque conocemos y le damos un valor de 20 330.0 Como se puede ver el resultado de la regresión nos dio un valor de 330.0. Pero será este resultado lo que realmente estabamos buscando?, será que el modelo me predijo correctamente lo que se queria?. una forma de averiguarlo es creando un conjunto de datos historicos sinteticos a los cuales les llamaremos \u0026ldquo;reales\u0026rdquo;. Recordemos que todo sistema de machine learning funciona con datos historicos. Como estamos trabajando con un sistema supervisado, recrearemos unos valores para W y b ideales para asi poder tener nuestros datos historicos simulados.\nEste es el punto mas importante los datos de w y b no los conocemos si los supieramos no necesitariamos algoritmos de aprendizaje, es por eso que en este ejemplo los idealizamos como si los supieramos para entender el por que es importante la etapa de entrenamiento.\n#datos ideales TRUE_W=3.0 TRUE_b=0.5 Para tener un conjunto de datos sinteticos mas reales adicionaremos ruido a nuestros conjunto de datos, esto simulara efecto de datos perdidos en el dataset, datos extremos, datos nulos etc.\nnoise =tf.random.normal(shape=(Numero_muestras,)) Con lo anterior creamos nuestro conjunto de datos historico simulado\ny=X*TRUE_W+TRUE_b + noise Ahora si, a lo que vinimos, vamos a crear un grafico donde pondremos los datos historicos reales y los datos que el modelo predice. Lo ideal seria encontrar una linea de regresion que este bien por encima de los datos originales\nimport matplotlib.pyplot as plt #grafica de los valores historicos reales plt.scatter(X,y, label=\u0026#34;valores reales\u0026#34;) #grafica de los valores predichos por el modelo plt.scatter(X,modelo(X), label = \u0026#34;valores predichos\u0026#34;) \u0026lt;matplotlib.collections.PathCollection at 0x7f18e6a00a58\u0026gt; los puntos en azul representan los valores reales, la linea de regresion naranja son los predichos por el modelo. Pero por que estan tan lejos una de la otra? esto se debe a que los pesos no se actualizaron.\nEse es nuestro objetivo actualizar los pesos sucesivamente hasta encontrar los que mejor adapten la linea de regresión a los datos historicos. En este caso nuestro modelo deberiá haber encontrado unos pesos de W=3.0 y b=0.5 esto no se consiguio debido al hecho de que en el modelo de regresión creado los valores de W y b se dejaron fijos en 16.0 y 10.0 respectivamente.\nEs por eso que el entrenamiento de los modelos es de suma importancia en todo algorimto de aprendizaje y conceptos como función de coste, errore medio cuadratico, descenso del gradiente entre otros empiezan a aparecer para facilitar la vida al momento de predecir.\n#Entrenamiento del modelo de regresión\nprint(\u0026#39;valores reales \u0026#39;,X[:4].numpy(),\u0026#39;\\nValores predichos \u0026#39;,modelo(X)[:4].numpy()) valores reales [-0.98775786 -0.4940117 -1.1781967 -0.46405175] Valores predichos [-5.804126 2.0958128 -8.851147 2.575172 ] Como se puede observar el error entre los valores reales y los valores predichos por el modelo son demasiado grandes es por eso que la grafica a quedado tan separada una de la otra. Con el fin de ajustar los valores y reducir esa discrepancia entre valores reales y predichos, se usan tecnicas que minimicen esos errores y son conocidas como funciones de coste J, que para el caso de la regresión lineal se tiene la del error minimo cuadrado:\n$J = (y_{real} - y_{predicho})^{2}$\ndef error_medio_cuadratico(y_real,y_predicho): return tf.reduce_mean(tf.square(y_real-y_predicho)) # error Como la tasa de errores se obtiene haciendo la diferencia de cada valor, se aplica la media de todos los errores de Y_real y Y_predicho con el objetivo de tener un solo valor que es el que se ira a sumar o restar a los pesos y bias para que estos se actualicen. seriá algo asi\n$W (+/-) error $ y $ b (+/-) error$\nPero como saber si se debe sumar o restar el erro a los pesos, es aqui donde entra una de los conceptos mas interesantes del aprendizaje de maquinas que es el calculo de la derivada de la funcion de coste con respecto a los pesos. Uno de los metodos mas usados para realizar esto es la optimización del gradiente descendiente. Con esta técnica conseguimos definir dos cosas\nLa direccion del cambio, saber si debo aumentar o disminuir los pesos La magnitud del cambio, saber si se disminuye mucho o se aumenta mucho el valor El primer item se soluciona con el calculo del gradiente, ya para el segundo es necesario adicionar un parámetro mas a nuestro algoritmmo de aprendizaje que es la tasa de aprendizaje que generalmente esta configurada entre 0 y 1.\nCalcularemos el gradiente con las funciones ofrecidas por tensorflow.\ndef entrenamiento(modelo,X,y,tasa_aprendizaje=0.01): with tf.GradientTape() as t: error_actual=error_medio_cuadratico(y ,modelo(X)) derivada_W, derivada_b = t.gradient(error_actual,[modelo.W,modelo.b]) modelo.W.assign_sub(tasa_aprendizaje*derivada_W) modelo.b.assign_sub(tasa_aprendizaje*derivada_b) Ahora con nuestra función de entrenamiento que contiene la actualizacion de pesos podremos mejorar el cálculo de nuestro modelo de regresión, para eso crearemos un loop con un determinado número de iteraciones para que los pesos se actualicen correctamente\n# en estas variables guardaremos los valores de los pesos de cada pasada ws,bs = [],[] numero_itraciones=10 for iteracion in range(numero_itraciones): ws.append(modelo.W.numpy()) bs.append(modelo.b.numpy()) error_actual = error_medio_cuadratico(y,modelo(X)) entrenamiento(modelo,X,y,tasa_aprendizaje=0.1) print(\u0026#34;Iteración {}, perdida {}\u0026#34;.format(iteracion, error_actual.numpy())) Iteración 0, perdida 255.19960021972656 Iteración 1, perdida 165.8444366455078 Iteración 2, perdida 107.90821838378906 Iteración 3, perdida 70.342041015625 Iteración 4, perdida 45.98304748535156 Iteración 5, perdida 30.187406539916992 Iteración 6, perdida 19.94432830810547 Iteración 7, perdida 13.301708221435547 Iteración 8, perdida 8.9938383102417 Iteración 9, perdida 6.2000041007995605 Como se puede observar la error despues de varias iteraciones disminuye considerablemnte eso quieres decir que la red esta aprendiendo a ajustar los pesos y generalizarlos a los ideales\nimport matplotlib.pyplot as plt #grafica de los valores historicos reales plt.scatter(X,y, label=\u0026#34;valores reales\u0026#34;) #grafica de los valores predichos por el modelo plt.scatter(X,modelo(X), label = \u0026#34;valores predichos\u0026#34;) \u0026lt;matplotlib.collections.PathCollection at 0x7f18ddeda3c8\u0026gt; Ahora se puede observar que los puntos de la linea azul se aproximan bastante a la linea originial naranja. es por eso que es importante la etapa de entrenamiento en modelos de machine learning.\nSin embargo, este es un modelo de regresión para funciones linealmente separables como el caso de la compuerta and pero si el problema representa funciones no linealmente separables, sera que un solo perceptron es suficiente para hacer los calculos.\nLa respuesta es no, para eso se conectan multiples neurones en multiples capas intercaonectados unos con otros formando lo que conocemos como redes neuronales. Que será explicado en el notebook Machine Learning II\n","date":1588633200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588633200,"objectID":"63bb1d9205eb07ffb32e4552044243ba","permalink":"http://carloshinojosa.me/courses/deep-learning-labs/concepts/","publishdate":"2020-05-05T00:00:00+01:00","relpermalink":"/courses/deep-learning-labs/concepts/","section":"courses","summary":"Antes de inicar con conceptos básicos sobre redes neuronales tensorflow keras etc, estudiaremos las ideas fundamentales sobre el mundo de la inteligencia artificial.\nLa inteligencia artificial se puede definir como la ciencia de entrenar máquinas para ejecutar tareas humanas y machine learning es una parte de la inteligencia artifical encargada de resolver esos problemas.\nImage(\u0026#39;ML.PNG\u0026#39;, width = 800) Machine Learning es un metodo de análisis para la construcción de modelos automatas, que usan diversos algoritmos para aprender iterativamente de un conjunto de datos historicos.","tags":null,"title":"Machine Learning I","type":"docs"},{"authors":[],"categories":null,"content":" Slides can be added in a few ways:\nCreate slides using academia\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"8ebbbdeba622fbc24dab29b4efa6b173","permalink":"http://carloshinojosa.me/talk/coding/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/coding/","section":"talk","summary":"An example talk using academia's Markdown slides feature.","tags":[],"title":"Coding and Analyzing Qualitative Data","type":"talk"},{"authors":[],"categories":null,"content":" Slides can be added in a few ways:\nCreate slides using academia\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a3975af5f5dadc9b2c7bbc4e48bb0e6e","permalink":"http://carloshinojosa.me/talk/qualitative/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/qualitative/","section":"talk","summary":"An example talk using academia's Markdown slides feature.","tags":[],"title":"Qualitative Research Summer Intensive","type":"talk"},{"authors":[],"categories":null,"content":" Slides can be added in a few ways:\nCreate slides using academia\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"f917008f74aa8012979052dcf8dbf864","permalink":"http://carloshinojosa.me/talk/synthesizing/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/synthesizing/","section":"talk","summary":"An example talk using academia's Markdown slides feature.","tags":[],"title":"Synthesizing Qualitative Data","type":"talk"},{"authors":["Karen Sanchez","Carlos Hinojosa","Henry Arguello","Denis Kouamé","Olivier Meyrignac","Adrian Basarab"],"categories":null,"content":"","date":1663100115,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663100115,"objectID":"1120723e4a4c9d42ec13336bc60d02cc","permalink":"http://carloshinojosa.me/publication/journal-ieee-tmi-da-chestxray-2022/","publishdate":"2022-09-13T15:15:15-05:00","relpermalink":"/publication/journal-ieee-tmi-da-chestxray-2022/","section":"publication","summary":"Recent advances in deep learning led to several algorithms for the accurate diagnosis of pneumonia from chest X-rays. However, these models require large training medical datasets, which are sparse, isolated, and generally private. Furthermore, these models in medical imaging are known to over-fit to a particular data domain source, i.e., these algorithms do not conserve the same accuracy when tested on a dataset from another medical center, mainly due to image distribution discrepancies. In this work, a domain adaptation and classification technique is proposed to overcome the over-fit challenges on a small dataset. This method uses a private-small dataset (target domain), a public-large labeled dataset from another medical center (source domain), and consists of three steps. First, it performs a data selection of the source domain's most representative images based on similarity constraints through principal component analysis subspaces. Second, the selected samples from the source domain are fit to the target distribution through an image to image translation based on a cycle-generative adversarial network. Finally, the target train dataset and the adapted images from the source dataset are used within a convolutional neural network to explore different settings to adjust the layers and perform the classification of the target test dataset. It is shown that fine-tuning a few specific layers together with the selected-adapted images increases the sorting accuracy while reducing the trainable parameters. The proposed approach achieved a notable increase in the target dataset's overall classification accuracy, reaching up to 97.78% compared to 90.03% by standard transfer learning.","tags":["Deep Learning","Transfer Learning","Domain Adaptation","Generative Adversarial Networks","Pneumonia Diagnosis"],"title":"CX-DaGAN: Domain Adaptation for Pneumonia Diagnosis on a Small Chest X-ray Dataset","type":"publication"},{"authors":["Paula Arguello","Jhon Lopez","Carlos Hinojosa","Henry Arguello"],"categories":null,"content":"","date":1663027200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663027200,"objectID":"c8b6c2a0cfb78816c169dff0b5b7ffdf","permalink":"http://carloshinojosa.me/publication/conf-icip2022/","publishdate":"2022-09-13T00:00:00Z","relpermalink":"/publication/conf-icip2022/","section":"publication","summary":"Image captioning is a challenging task that connects two major artificial intelligence fields: computer vision and natural language processing. Image captioning models use traditional images to generate a natural language description of the scene. However, the scene could contain private information that we want to hide but still generate the captions. Inspired by the trend of jointly designing optics and algorithms, this paper addresses the problem of privacy-preserving scene captioning. Our approach promotes privacy preservation, by hiding the faces in the images, during the acquisition process with a designed refractive camera lens while extracting useful features to perform image captioning. The refractive lens and an image captioning deep network architecture are optimized end-to-end to generate descriptions directly from the blurred images. Simulations show that our privacy-preserving approach degrades private visual attributes (e.g., face detection fails with our distorted images) while achieving comparable captioning performance with traditional non-private methods on the COCO dataset. See our [Project Page](https://carloshinojosa.me/project/privacy-captioning/)!","tags":["Privacy Preserving","Computational Photography","Computational Imaging","Image Captioning","Computer Vision"],"title":"Optics lens design for privacy-preserving scene captioning","type":"publication"},{"authors":["Miguel Marquez","Jonathan Monsalve","Kevin Arias","Karen Sanchez","Carlos Hinojosa","Henry Arguello"],"categories":null,"content":"","date":1662681600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662681600,"objectID":"76d78190b8f27034cfb047de91d643af","permalink":"http://carloshinojosa.me/publication/conf-whispers2022-singlepixel-clustering/","publishdate":"2022-09-09T00:00:00Z","relpermalink":"/publication/conf-whispers2022-singlepixel-clustering/","section":"publication","summary":"This paper proposes a hierarchical approach to design the sensing matrix of the SPC, such that the pixel clustering task can be performed directly using the compressed infrared SPC measurements without a previous reconstruction step. Specifically, a sensing matrix is designed to extract features directly from the compressed measurements at each hierarchy step. Then, a final segmentation map is obtained through majority voting in the partial clustering results.","tags":["Single-pixel"],"title":"Hierarchical Compressed Subspace Clustering of Infrared Single-pixel Measurements","type":"publication"},{"authors":null,"categories":null,"content":"","date":1658102400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1658102400,"objectID":"f1c0055b503a53d4543ab4983524240b","permalink":"http://carloshinojosa.me/project/privhar/","publishdate":"2022-07-18T00:00:00Z","relpermalink":"/project/privhar/","section":"project","summary":"Landing page of our ECCV 2022 Paper.","tags":null,"title":"PrivHAR: Recognizing Human Actions From Privacy-preserving Lens","type":"widget_page"},{"authors":["Carlos Hinojosa","Esteban Vera","Henry Arguello"],"categories":null,"content":"","date":1634760915,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634760915,"objectID":"05c3050172e8ae5913c0e62b79cbc5bb","permalink":"http://carloshinojosa.me/publication/journal-ieee-jstar-sc-ssc-2021/","publishdate":"2021-10-20T15:15:15-05:00","relpermalink":"/publication/journal-ieee-jstar-sc-ssc-2021/","section":"publication","summary":"In this paper, we propose a fast algorithm that obtains a sparse representation coefficient matrix by first selecting a small set of pixels that best represent their neighborhood. Then, it performs spatial filtering to enforce the connectivity of neighboring pixels and uses fast spectral clustering to get the final clustering map.","tags":["Hyperspectral image clustering","Clustering algorithms","Unsupervised Classification","Subspace Clustering","Unsupervised learning"],"title":"A Fast and Accurate Similarity-constrained Subspace Clustering Algorithm for Hyperspectral Image","type":"publication"},{"authors":["Carlos Hinojosa","Juan Carlos Niebles","Henry Arguello"],"categories":null,"content":"","date":1626998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626998400,"objectID":"131dc127745452a900940bec8f6a2f5d","permalink":"http://carloshinojosa.me/publication/conf-iccv2021/","publishdate":"2021-07-23T00:00:00Z","relpermalink":"/publication/conf-iccv2021/","section":"publication","summary":"The widespread use of always-connected digital cameras in our everyday life has led to increasing concerns about the users privacy and security. How to develop privacy-preserving computer vision systems? In particular, we want to prevent the camera from obtaining detailed visual data that may contain private information. However, we also want the camera to capture useful information to perform computer vision tasks. Inspired by the trend of jointly designing optics and algorithms, we tackle the problem of privacy-preserving human pose estimation by optimizing an optical encoder (hardware-level protection) with a software decoder (convolutional neural network) in an end-to-end framework. We introduce a visual privacy protection layer in our optical encoder which, parametrized appropriately, enables the optimization of the point spread function (PSF) of the camera lens. We validate our approach with extensive simulations and a prototype camera. We show that our privacy-preserving deep optics approach successfully degrades or inhibits private attributes while maintaining important features to perform human pose estimation. See our [Project Page](https://carloshinojosa.me/project/privacy-hpe/)!","tags":["Privacy Preserving","Computational Photography","Computational Imaging","Human Pose Estimation","Computer Vision"],"title":"Learning Privacy-preserving Optics for Human Pose Estimation.","type":"publication"},{"authors":["Jhon López","Carlos Hinojosa","Henry Arguello"],"categories":null,"content":"","date":1626466569,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626466569,"objectID":"e2c77a8abb483579985115fc3710e5ef","permalink":"http://carloshinojosa.me/publication/journal-spie-jars-efficient-f4sc/","publishdate":"2021-07-16T15:16:09-05:00","relpermalink":"/publication/journal-spie-jars-efficient-f4sc/","section":"publication","summary":"The unsupervised classification of hyperspectral images (HSIs) draws attention in the remote sensing community due to its inherent complexity and the lack of labeled data. Among unsupervised methods, sparse subspace clustering (SSC) achieves high clustering accuracy by constructing a sparse affinity matrix. However, SSC has limitations when clustering HSI images due to the number of spectral pixels. Specifically, the temporal complexity grows at a cubic ratio of the size of the data, making it inefficient for addressing HSI subspace clustering. We propose an efficient SSC-based method that significantly reduces the temporal and spatial computational complexity by splitting the HSI clustering task using similarity-constrained sampling. Our similarity-constrained sampling strategy considers both edge and superpixel information of the HSI to boost the clustering performance. This sampling strategy enables an intelligent selection of spectral signatures, and then, we split the clustering problem into multiples threads. Experimental results on widely used HSI datasets show that the efficiency of the proposed method outperforms baseline methods by up to 30% in overall accuracy and up to six times in computing time.","tags":["Spectral Image Classification","Hyperspectral image segmentation","Subspace Clustering","Unsupervised Classification"],"title":"Efficient subspace clustering of hyperspectral images using similarity-constrained sampling","type":"publication"},{"authors":null,"categories":null,"content":"","date":1626393600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626393600,"objectID":"38f9f618f0a9f841824c13d63579593c","permalink":"http://carloshinojosa.me/project/privacy-hpe-project-page/","publishdate":"2021-07-16T00:00:00Z","relpermalink":"/project/privacy-hpe-project-page/","section":"project","summary":"In this project, we design the camera lens to perform human pose estimation while preserving users’ privacy.","tags":["Computer vision"],"title":"Learning Privacy-preserving Optics for Human Pose Estimation","type":"project"},{"authors":null,"categories":null,"content":"","date":1625097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625097600,"objectID":"af2201a4654d9af1d4bad49ff4e2acb1","permalink":"http://carloshinojosa.me/project/privacy-hpe/","publishdate":"2021-07-01T00:00:00Z","relpermalink":"/project/privacy-hpe/","section":"project","summary":"Landing page of our ICCV 2021 Paper.","tags":null,"title":"Learning Privacy-preserving Optics for Human Pose Estimation","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1625097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625097600,"objectID":"6e848174bc979c2468135e1d1f3e00fa","permalink":"http://carloshinojosa.me/project/privacy-captioning/","publishdate":"2021-07-01T00:00:00Z","relpermalink":"/project/privacy-captioning/","section":"project","summary":"Landing page of our ICIP 2022 Paper.","tags":null,"title":"Optics Lens Design for Privacy-preserving Scene Captioning","type":"widget_page"},{"authors":["Karen Sanchez","Carlos Hinojosa","Henry Arguello","Simon Freiss","Nicolas Sans","Denis Kouamé","Oliver Meyrignc","Adrian Basarab"],"categories":null,"content":"","date":1618272000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618272000,"objectID":"cf449f10bdf076d542e2a278362cf8ca","permalink":"http://carloshinojosa.me/publication/conf-isbi2021-subspace-domain-chest-xray/","publishdate":"2021-04-13T00:00:00Z","relpermalink":"/publication/conf-isbi2021-subspace-domain-chest-xray/","section":"publication","summary":"Recent advances in deep learning have led to an accurate diagnosis of pneumonia from chest X-ray images. However, these models usually require large labeled training datasets, not always available in practice. Furthermore, combining images from different medical centers does not preserve the accuracy of the results mainly because of differences in image acquisition settings. In this work, we propose an approach aiming to overcome this challenge, consisting of a subspace-based domain adaptation technique to increase pneumonia detection accuracy using a small training dataset. This dataset is augmented with automatically selected images from a large dataset acquired in a different medical center. This is performed by computing a subspace basis of the target domain dataset on which is projected the source dataset to find the most representative images. Augmenting the training set using the proposed method allows achieving an improvement from 90.03% to 96.18% in overall accuracy using the Xception neural network.","tags":["Deep Learning","Transfer Learning","Subspace Domain Adaptation","Neural Networks","Generative Adversarial Networks"],"title":"Subspace-based Domain Adaptation Using Similarity Constraints for Pneumonia Diagnosis within a Small Chest X-ray Image Dataset","type":"publication"},{"authors":["Carlos Hinojosa","Fernando Rojas","Sergio Castillo","Henry Arguello"],"categories":null,"content":"","date":1611864969,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611864969,"objectID":"8e8be54161e30c455d2420a815b80927","permalink":"http://carloshinojosa.me/publication/journal-spie-jars-3ds-ssc/","publishdate":"2021-01-28T15:16:09-05:00","relpermalink":"/publication/journal-spie-jars-3ds-ssc/","section":"publication","summary":"The accurate segmentation of remotely sensed hyperspectral images has widespread attention in the Earth observation and remote sensing communities. In the past decade, most of the efforts focus on the development of different supervised methods for hyperspectral image classification. Recently, the computer vision community is developing unsupervised methods that can adapt to new conditions without leveraging expensive supervision. In general, among unsupervised classification methods, sparse subspace clustering (SSC) is a popular tool that achieves good clustering results on experiments with real data. However, for the specific case of hyperspectral clustering, the SSC model does not take into account the spatial information of such images, which limits its discrimination capability and hampering the spatial homogeneity of the clustering results. As a solution, we propose to incorporate a regularization term to the SSC model, which takes into account the neighboring spatial information of spectral pixels in the scene. Specifically, the proposed method uses a three-dimensionall (3D) Gaussian filter to perform a 3D convolution on the sparse coefficients, obtaining a piecewise-smooth representation matrix that enforces an averaging constraint in the SSC optimization program. Extensive simulations demonstrate the effectiveness of the proposed method, achieving an overall accuracy of up to 99% in the selected hyperspectral remote sensing datasets.","tags":["Spectral Image Classification","Hyperspectral image segmentation","Subspace Clustering","Unsupervised Classification","Spatio-spectral classification"],"title":"Hyperspectral image segmentation using 3D regularized subspace clustering model","type":"publication"},{"authors":[],"categories":[],"content":"\nAntes de empezar, da click en el boton \u0026ldquo;Open in Colab\u0026rdquo; de arriba. Esto abrirá este notebook de python directamente en Colab. Luego, para poder editar y ejecutar el código, deberas copiar el notebook en Google drive utilizando el boton:\nDe esta manera, este notebook quedará almacenado en Google drive, en la carpeta Colab Notebooks\nIntroducción El objetivo de este tutorial es proporcionar un flujo de trabajo para entrenar modelos de Deep Learning. El flujo de trabajo propuesto requiere tener instalado los siguientes componentes en nuestra computadora:\nCliente de sincronización de Google Drive. Editor de texto o IDE de python como Pycharm. La idea general será tener nuestro código python en una carpeta en Google Drive. De esta manera, utilizaremos el cliente de sincronización para modificar los archivos de manera local en nuestras computadoras (utilizando el editor de texto o un IDE de python) y que los cambios se vean reflejados directamente en Colab. Aunque Colab permite editar archivos (simplemente dando doble click en el archivo deseado en el menu de la izquierda) es mucho mas sencillo y cómodo editar y manipular archivos complejos en un IDE de python.\nCliente Google Drive Para sistemas operativos Windows y Mac el cliente de Google drive puede ser descargado directamente de la pagina de google drive\nhttps://www.google.com/drive/download/\nSin embargo, para sistemas operativos Linux no existe un cliente oficial de google drive, pero existen alternativas excelentes como Insync (Software pago una única vez) o el cliente para sistemas Ubuntu:\nhttps://cambiatealinux.com/instalar-google-drive-en-ubuntu\nEditor de Python Para trabajar con proyectos Python, es recomendable utilizar el entorno de desarrollo integrado (IDE) Pycharm. Actualmente, es posible acceder a una licencia de estudiante de la version profesional de Pycharm por un año:\nhttps://www.jetbrains.com/es-es/community/education/#students\nPara acceder al beneficio solo se necesita el correo electronico institucional (@correo.uis.edu.co o @saber.uis.edu.co o @uis.edu.co).\nSi no se desea utilizar un IDE, es posible trabajar con un editor avanzado de texto: Visual Studio Code:\nhttps://code.visualstudio.com/\nNote que, debido a que nuestra intención es solo modificar el código en nuestra computadora local y ejecutar el código en Colab, no necesitamos tener instalado Python y las librerias de Tensorflow en nuestra computadora ya que estas se encuentran instaladas en Colab. Sin embargo, es recomendable instalar Python (Anaconda) y configurarlo con Pycharm en nuestra computadora local. A continuación proporciono links de interes:\nInstalar Anaconda: Windows: https://docs.anaconda.com/anaconda/install/windows/, Linux: https://docs.anaconda.com/anaconda/install/linux/, Mac: https://docs.anaconda.com/anaconda/install/mac-os/\nConfigurar Pycharm con Anaconda: https://docs.anaconda.com/anaconda/user-guide/tasks/pycharm/\nInstalar liberias de Deep Learning: https://asociacionaepi.es/primeros-pasos-con-tensorflow/\nSolicitar Recursos a Colab Una vez creemos un notebook en Colab, para solicitar recursos debemos primero seleccionar el entorno de ejecución adecuado y dar click en el botón Conectar:\nUna vez conectados, Colab nos asigna un entorno Linux con 25.51 GB de Ram, 68 GB de disco duro y una GPU cuyas caracteristicas podemos consultar ejecutando la siguiente celda\nCaracterísticas de la GPU asignada # Check nvidia and nvcc cuda compiler !nvidia-smi !/usr/local/cuda/bin/nvcc --version Tue May 19 00:16:53 2020 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.82 Driver Version: 418.67 CUDA Version: 10.1 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 | | N/A 34C P0 26W / 250W | 0MiB / 16280MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2019 NVIDIA Corporation Built on Sun_Jul_28_19:07:16_PDT_2019 Cuda compilation tools, release 10.1, V10.1.243 Por lo general, Colab asigna de manera aleatoria GPUs con diferente cantidad de memoria. Es recomendable utilizar una GPU de 16 o 15 GB de memoria para proyectos que requieran mucha memoria o trabajen con muchos datos. En caso de que Colab no nos asigne una GPU adecuada (algunas veces asigna GPUs de 8 o 11 GB de memoria) para nuestro proyecto, siempre podemos realizar distintas solicitudes hasta conseguir una GPU adecuada. Para esto debemos cerrar la sesion actual y volver a solicitar recursos:\nEste proceso se repite varias veces hasta que se nos asigne la GPU deseada. Note que es posible que requiera recargar la ventana (F5) para poder finalizar la sesion. OJO Esto solo es necesario hacerlo si necesitamos una GPU de un tamaño de memoria especifico. Si nuestro proyecto es pequeño, cualquier GPU asignada por Colab servirá.\nNota: Este notebook está pre-configurado para trabajar con 25 Gb de memoria RAM, sin embargo esta configuración no es por defecto. La cantidad de memoria asignada por general es de 12 a 16 GB. Si crea un notebook nuevo en Colab y quiere tener 25 GB ver sección de Bonus al final de este tutorial.\nComandos Básicos Debido a que cada notebook de Colab se ejecuta en una máquina Linux, es posible utilizar todos los comandos de Linux. Los comandos más básicos para movernos dentro de este tipo de ambiente son:\n%pwd # Ver el directorio actual de trabajo %ls # Listar los archivos del directorio actual %cd # Cambiar de directorio %mkdir # Crear un nuevo directorio %rmdir # Eliminar un directorio vacio Por ejemplo, si quiero ver en que carpeta me encuentro actualmente ejecuto una celda con el comando:\n%pwd '/content' Como vemos, me encuentro en la carpeta \u0026lsquo;/content\u0026rsquo;, es decir, la carpeta raiz de Colab. Adicionalmente, podemos ejecutar varios comandos dentro de una celda agregando %%shell al inicio de esta. Por ejemplo:\n%%shell pwd ls mkdir \u0026#34;nuevo_directorio\u0026#34; ls rmdir \u0026#34;nuevo_directorio\u0026#34; ls /content sample_data nuevo_directorio sample_data sample_data Note que si se agrega %%shell al inicio, se debe quitar el simbolo % al principio de cada comando. Así, en vez de escribir %pwd, escribimos pwd solamente. Note ademas, que el resultado de cada comando se muestra en una linea aparte. De esta manera, el primer comando ls, muestra el contenido de la carpeta /content, que en este caso es solo la carpeta \u0026lsquo;sample_data/\u0026rsquo;. Luego creamos la carpeta \u0026rsquo;nuevo_directorio\u0026rsquo; (observe que el comando mkdir no arroja ninguna salida), listamos el contenido de content/ para ver la nueva carpeta creada y por ultimo borramos la carpeta.\nEn general se puede utilizar cualquier comando linux, incluso instalar paquetes linux con el software apt, por ejemplo:\n%%shell sudo apt install nano %%shell sudo apt install nano Reading package lists... Done Building dependency tree Reading state information... Done Suggested packages: spell The following NEW packages will be installed: nano 0 upgraded, 1 newly installed, 0 to remove and 31 not upgraded. Need to get 231 kB of archives. After this operation, 778 kB of additional disk space will be used. Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 nano amd64 2.9.3-2 [231 kB] Fetched 231 kB in 2s (130 kB/s) debconf: unable to initialize frontend: Dialog debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, \u0026lt;\u0026gt; line 1.) debconf: falling back to frontend: Readline debconf: unable to initialize frontend: Readline debconf: (This frontend requires a controlling tty.) debconf: falling back to frontend: Teletype dpkg-preconfigure: unable to re-open stdin: Selecting previously unselected package nano. (Reading database ... 144433 files and directories currently installed.) Preparing to unpack .../nano_2.9.3-2_amd64.deb ... Unpacking nano (2.9.3-2) ... Setting up nano (2.9.3-2) ... update-alternatives: using /bin/nano to provide /usr/bin/editor (editor) in auto mode update-alternatives: using /bin/nano to provide /usr/bin/pico (pico) in auto mode Processing triggers for man-db (2.8.3-2ubuntu0.1) ... Puede buscar mas comandos en internet, una pagina inicial sería:\nhttps://marcosmarti.org/comandos-basicos-de-linux/\nSetup Lo primero que haremos será conectarnos con nuestra cuenta de Google Drive. Importante conectarse a la cuenta de google drive en donde se tiene almacenado el proyecto o el código a ejecutar. Adicionalmente, esta cuenta debe estar previamente sincronizada en nuestra computadora utilizando un cliente de Google Drive (Ver Introducción). Para conectarnos a google drive, ejecutamos la celda de abajo y seguimos los pasos. Por favor seleccione la cuenta previamente sincronizada.\n##Mount Goolge Drive\n# link to google drive from google.colab import drive #drive.mount(\u0026#39;/content/gdrive/\u0026#39;) drive.mount(\u0026#34;/content/gdrive/\u0026#34;, force_remount=True) Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com\u0026amp;redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob\u0026amp;response_type=code\u0026amp;scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly Enter your authorization code: ·········· Mounted at /content/gdrive/ Si damos click en el panel de la izquierda, al icono de carpeta, veremos que tenemos una nueva carpeta llamada \u0026lsquo;gdrive\u0026rsquo;. De esta manera habremos sincronizado de manera correcta google drive con Colab.\nAhora la idea es trabajar en nuestro código. Existen básicamente dos maneras:\nAñadir el codigo directamente en este notebook e ir ejecutando cada celda. Ejecutar un script de python directamente desde una celda. En este tutorial se utilizará la segunda forma. Para esto utilicé como ejemplo el tutorial de tensorflow:\nhttps://www.tensorflow.org/tensorboard/get_started\nSeguidamente, creé un proyecto en mi computadora local llamado colab_tutorial. La carpeta de este proyecto se encuentra sincronizado con mi Google Drive, por lo tanto, todo cambio que se realice en mi computadora local se vera reflejado directamente en Drive y, de igual forma, en Colab.\nUna vez tengamos listo el código en nuestra carpeta local, solo basta con ejecutar una celda de la siguiente forma\n!python3 nombre_archivo.py --parametros (opcional) De esta forma ejecutaremos nuestro código en Colab. Note que nuestro código tambien puede generar salidas, las cuales (si se almacenan dentro de la misma carpeta del proyecto) quedarán sincronizadas con nuestra computadora local.\nAqui un ejemplo de ejecutar la funcion main.py dentro de mi carpeta proyecto \u0026ldquo;colab_tutorial\u0026rdquo;. Primero vamos hacia la carpeta con los comandos básicos explicados en la sección 2.\n%cd /content/gdrive/My\\ Drive/colab_tutorial/ /content/gdrive/My Drive/colab_tutorial %ls main.py !python main.py 2020-05-19 01:24:56.751869: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1 2020-05-19 01:24:58.852417: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1 2020-05-19 01:24:58.866042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-19 01:24:58.866658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0 coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s 2020-05-19 01:24:58.866696: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1 2020-05-19 01:24:58.868230: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10 2020-05-19 01:24:58.869913: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10 2020-05-19 01:24:58.870253: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10 2020-05-19 01:24:58.871814: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10 2020-05-19 01:24:58.872594: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10 2020-05-19 01:24:58.875551: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7 2020-05-19 01:24:58.875654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-19 01:24:58.876241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-19 01:24:58.876770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0 2020-05-19 01:24:58.881933: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200000000 Hz 2020-05-19 01:24:58.882377: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x138f100 initialized for platform Host (this does not guarantee that XLA will be used). Devices: 2020-05-19 01:24:58.882409: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version 2020-05-19 01:24:58.960753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-19 01:24:58.961607: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x138ef40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices: 2020-05-19 01:24:58.961643: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0 2020-05-19 01:24:58.961839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-19 01:24:58.962429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0 coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s 2020-05-19 01:24:58.962471: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1 2020-05-19 01:24:58.962526: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10 2020-05-19 01:24:58.962541: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10 2020-05-19 01:24:58.962554: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10 2020-05-19 01:24:58.962566: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10 2020-05-19 01:24:58.962581: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10 2020-05-19 01:24:58.962596: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7 2020-05-19 01:24:58.962655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-19 01:24:58.963213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-19 01:24:58.963746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0 2020-05-19 01:24:58.963791: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1 2020-05-19 01:24:59.498013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix: 2020-05-19 01:24:59.498074: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108] 0 2020-05-19 01:24:59.498084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0: N 2020-05-19 01:24:59.498276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-19 01:24:59.498864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-19 01:24:59.499412: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0. 2020-05-19 01:24:59.499466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14973 MB memory) -\u0026gt; physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0) 2020-05-19 01:24:59.547390: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session started. 2020-05-19 01:24:59.547453: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1363] Profiler found 1 GPUs 2020-05-19 01:24:59.548374: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcupti.so.10.1 2020-05-19 01:24:59.678606: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1479] CUPTI activity buffer flushed Epoch 1/10 2020-05-19 01:25:00.110434: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10 2020-05-19 01:25:00.407164: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session started. 1/1875 [..............................] - ETA: 0s - loss: 2.5025 - accuracy: 0.0000e+002020-05-19 01:25:00.411242: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1479] CUPTI activity buffer flushed 2020-05-19 01:25:00.411386: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:216] GpuTracer has collected 62 callback api events and 62 activity events. 2020-05-19 01:25:00.424149: I tensorflow/core/profiler/rpc/client/save_profile.cc:168] Creating directory: logs/fit/20200519-012459/train/plugins/profile/2020_05_19_01_25_00 2020-05-19 01:25:00.430074: I tensorflow/core/profiler/rpc/client/save_profile.cc:174] Dumped gzipped tool data for trace.json.gz to logs/fit/20200519-012459/train/plugins/profile/2020_05_19_01_25_00/998f21424347.trace.json.gz 2020-05-19 01:25:00.430893: I tensorflow/core/profiler/utils/event_span.cc:288] Generation of step-events took 0.016 ms 2020-05-19 01:25:00.449145: I tensorflow/python/profiler/internal/profiler_wrapper.cc:87] Creating directory: logs/fit/20200519-012459/train/plugins/profile/2020_05_19_01_25_00Dumped tool data for overview_page.pb to logs/fit/20200519-012459/train/plugins/profile/2020_05_19_01_25_00/998f21424347.overview_page.pb Dumped tool data for input_pipeline.pb to logs/fit/20200519-012459/train/plugins/profile/2020_05_19_01_25_00/998f21424347.input_pipeline.pb Dumped tool data for tensorflow_stats.pb to logs/fit/20200519-012459/train/plugins/profile/2020_05_19_01_25_00/998f21424347.tensorflow_stats.pb Dumped tool data for kernel_stats.pb to logs/fit/20200519-012459/train/plugins/profile/2020_05_19_01_25_00/998f21424347.kernel_stats.pb 1875/1875 [==============================] - 5s 3ms/step - loss: 0.2168 - accuracy: 0.9365 - val_loss: 0.1001 - val_accuracy: 0.9694 Epoch 2/10 1875/1875 [==============================] - 5s 3ms/step - loss: 0.0948 - accuracy: 0.9710 - val_loss: 0.0865 - val_accuracy: 0.9725 Epoch 3/10 1875/1875 [==============================] - 5s 3ms/step - loss: 0.0670 - accuracy: 0.9786 - val_loss: 0.0708 - val_accuracy: 0.9778 Epoch 4/10 1875/1875 [==============================] - 5s 2ms/step - loss: 0.0525 - accuracy: 0.9832 - val_loss: 0.0623 - val_accuracy: 0.9810 Epoch 5/10 1875/1875 [==============================] - 5s 2ms/step - loss: 0.0433 - accuracy: 0.9855 - val_loss: 0.0696 - val_accuracy: 0.9806 Epoch 6/10 1875/1875 [==============================] - 5s 2ms/step - loss: 0.0357 - accuracy: 0.9885 - val_loss: 0.0685 - val_accuracy: 0.9816 Epoch 7/10 1875/1875 [==============================] - 5s 2ms/step - loss: 0.0299 - accuracy: 0.9901 - val_loss: 0.0625 - val_accuracy: 0.9822 Epoch 8/10 1875/1875 [==============================] - 5s 2ms/step - loss: 0.0276 - accuracy: 0.9908 - val_loss: 0.0655 - val_accuracy: 0.9821 Epoch 9/10 1875/1875 [==============================] - 5s 2ms/step - loss: 0.0237 - accuracy: 0.9925 - val_loss: 0.0789 - val_accuracy: 0.9806 Epoch 10/10 1875/1875 [==============================] - 5s 2ms/step - loss: 0.0230 - accuracy: 0.9923 - val_loss: 0.0750 - val_accuracy: 0.9827 En este ejemplo en particular se hizo uso de la herramienta Tensorboard, el cual proporciona una interfaz de visualización para el entrenamiento de los modelos. Al ejecutar nuestro código, se guarda en la carpeta logs el resultado de entrenamiento para cada epoch. Para visualizar la herramienta Tensorboard tenemos basicamente dos formas:\nVisualizar Tensorboard directamente en Colab Visualizar Tensorboard en nuestra computadora local La desventaja de visualizar en Colab es que debemos esperar a que termine el entrenamiento para visualizar Tensorboard. Por el contrario, debido a que tenemos nuestra carpeta sincronizada, podemos ejecutar Tensorboard en nuestra computadora y visualizar el entrenamiento de manera local en tiempo real mientras se ejecuta el entrenamiento en Colab.\nPara visualizar el resultado directamente en este notebook ejecutamos las siguientes dos celdas. La primera solo se ejecuta una vez, pues es la encargada de cargar la extension.\n# Load the TensorBoard notebook extension (Ejecutar una sola vez) %load_ext tensorboard %tensorboard --logdir logs/fit Reusing TensorBoard on port 6006 (pid 1921), started 0:04:55 ago. (Use '!kill 1921' to kill it.) \u0026lt;IPython.core.display.Javascript object\u0026gt; Los detalles de implementación del código utilizado de ejemplo se pueden consultar directamente en la página del tutorial: https://www.tensorflow.org/tensorboard/get_started\n#Conclusion En conclusion, gracias a la sincronización que nos provee el cliente de google drive, podemos hacer todas las modificaciones necesarias de manera local en nuestras computadoras y unicamente utilizar colab para ejecturar el entrenamiento de la red neuronal aprovechando su GPU. Adicionalmente, el uso de la herramienta Tensorboard es fundamental para monitorear el entrenamiento.\nBonus 1) Cuando ejecutamos entrenamientos muy largos, es posible que Colab se desconecte inesperadamente debido a falta de interactividad. Para solucionar este problema podemos agregar un código Javascript en esta página para hacer \u0026ldquo;clicks\u0026rdquo; de manera automática en el boton Conect (Boton donde se muestra uso de RAM y Disco). El código en cuestión es el siguiente:\nfunction ClickConnect(){ console.log(\u0026#34;Clicked on connect button\u0026#34;); document.querySelector(\u0026#34;colab-connect-button\u0026#34;).click() } setInterval(ClickConnect,60000) Este código debe agregarse en la consola de Javascript de la siguiente manera:\n2) Si se quiere mas memoria ram, se puede ejecutar el siguiente código en una celda:\na = [] while(1): a.append(‘1’) Al ejecutarlo, este comando va a llenar la memoria ram disponible en Colab y forzará a este a ampliar la capacidad. Debemos esperar aproximadamente 1 minuto hasta que salga un letrero que pregunta si deseamos ampliar la memoria. Despues de aceptar, tendremos más memoria RAM en nuestro notebook. Mas información: https://towardsdatascience.com/upgrade-your-memory-on-google-colab-for-free-1b8b18e8791d\n","date":1590879878,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590879878,"objectID":"2a7e88d4d75649e8a55739325e0a2ac9","permalink":"http://carloshinojosa.me/post/colab-intro1/","publishdate":"2020-05-30T18:04:38-05:00","relpermalink":"/post/colab-intro1/","section":"post","summary":"El objetivo de este tutorial es proporcionar un flujo de trabajo para entrenar modelos de Deep Learning. La idea general será utilizar el cliente de sincronización de Google Drive para modificar los archivos de manera local en nuestras computadoras y que los cambios se vean reflejados directamente en Colab.","tags":["colab","pycharm","deep-learning"],"title":"Flujo de trabajo con Colab y Pycharm","type":"post"},{"authors":null,"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"1f4cb24553577f6808a73065326a8b9d","permalink":"http://carloshinojosa.me/blog/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/blog/","section":"","summary":"Carlos Hinojosa Blog","tags":null,"title":"Blog","type":"widget_page"},{"authors":["Carlos Hinojosa","Jorge Bacca","Edwin Vargas","Sergio Castillo","Henry Arguello"],"categories":null,"content":"","date":1571875200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571875200,"objectID":"d4b2de8076d5a1779f748dbba1b8db3b","permalink":"http://carloshinojosa.me/publication/conf-mlsp2019/","publishdate":"2019-10-24T00:00:00Z","relpermalink":"/publication/conf-mlsp2019/","section":"publication","summary":"In this work, we propose a hierarchical adaptive approach to design a sensing matrix of the single-pixel camera,  such that pixel clustering can be performed directly on the compressed domain. We obtain overall accuracies of 79%, and 65%.","tags":["Compressive spectral clustering","Single pixel camera","Hierarchical clustering","Matrix design"],"title":"Single-pixel camera sensing matrix design for hierarchical compressed spectral clustering","type":"publication"},{"authors":["Carlos Hinojosa","Juan Marcos Ramirez","Henry Arguello"],"categories":null,"content":"","date":1569369600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569369600,"objectID":"3cea7dfd9c3376ecdf48a91d26b5c071","permalink":"http://carloshinojosa.me/publication/conf-icip2019/","publishdate":"2019-08-26T00:00:00Z","relpermalink":"/publication/conf-icip2019/","section":"publication","summary":"In this work, we propose to obtain features by considering the spectral information extracted from Hyperspectral CSI measurements, and the local spatial information extracted by clustering the Multispectral CSI measurements.","tags":["Spectral Image Classification","Superpixels Algorithms","Compressive Spectral Imaging","Multi-sensor Measurements"],"title":"Spectral-spatial classification from multi-sensor compressive measurements using superpixels.","type":"publication"},{"authors":["Nelson Diaz","Carlos Hinojosa","Henry Arguello"],"categories":null,"content":"","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567296000,"objectID":"b07f9909ff3cf8ca0a61fafed6587fb8","permalink":"http://carloshinojosa.me/publication/journal-adaptive2019/","publishdate":"2019-04-16T00:00:00Z","relpermalink":"/publication/journal-adaptive2019/","section":"publication","summary":"This paper proposes an adaptive grayscale coded aperture design which combines the advantages of blue noise and block-unblock coding patterns to provide high-quality image reconstructions and redundancy in the sampling.","tags":["Compressive spectral imaging","Spectral imaging systems","Coded aperture design","Grayscale coded aperture","Adaptive imaging","Computational imaging"],"title":"Adaptive grayscale compressive spectral imaging using optimal blue noise coding patterns","type":"publication"},{"authors":["Carlos Hinojosa"],"categories":[],"content":"from IPython.core.display import Image Image(\u0026#39;https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png\u0026#39;) print(\u0026#34;Welcome to academia!\u0026#34;) Welcome to academia! Install Python and Jupyter Install Anaconda which includes Python 3 and Jupyter notebook.\nOtherwise, for advanced users, install Jupyter notebook with pip3 install jupyter.\nCreate a new blog post as usual Run the following commands in your Terminal, substituting \u0026lt;MY_WEBSITE_FOLDER\u0026gt; and my-post with the file path to your academia website folder and a name for your blog post (without spaces), respectively:\ncd \u0026lt;MY_WEBSITE_FOLDER\u0026gt; hugo new --kind post post/my-post cd \u0026lt;MY_WEBSITE_FOLDER\u0026gt;/content/post/my-post/ Create or upload a Jupyter notebook Run the following command to start Jupyter within your new blog post folder. Then create a new Jupyter notebook (New \u0026gt; Python Notebook) or upload a notebook.\njupyter notebook Convert notebook to Markdown jupyter nbconvert Untitled.ipynb --to markdown --NbConvertApp.output_files_dir=. # Copy the contents of Untitled.md and append it to index.md: cat Untitled.md | tee -a index.md # Remove the temporary file: rm Untitled.md Edit your post metadata Open index.md in your text editor and edit the title etc. in the front matter according to your preference.\nTo set a featured image, place an image named featured into your post\u0026rsquo;s folder.\nFor other tips, such as using math, see the guide on writing content with academia.\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"http://carloshinojosa.me/post/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/jupyter/","section":"post","summary":"Learn how to blog in academia using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with academia","type":"post"},{"authors":["Karen Sanchez","Carlos Hinojosa","Henry Arguello"],"categories":null,"content":"","date":1546992000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546992000,"objectID":"5f0efce14986617f6d80227d6b610220","permalink":"http://carloshinojosa.me/publication/journal-rgb-superpixels/","publishdate":"2019-02-19T00:00:00Z","relpermalink":"/publication/journal-rgb-superpixels/","section":"publication","summary":"The proposed approach exploits the spatial information of an RGB image by grouping pixels with similar characteristics into superpixels and fuses such features with the spectral information of an HS image.","tags":["Spectral Image Fusion","Spectral imaging systems","Spatio-spectral classification","Superpixels","Supervised Classification"],"title":"Supervised spatio-spectral classification of fused images using superpixels","type":"publication"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"3960dd3bdc6f629fb800d1d2aaa7224f","permalink":"http://carloshinojosa.me/resume/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/resume/","section":"","summary":"Carlos Hinojosa Online Resume","tags":null,"title":"Resume","type":"widget_page"},{"authors":["Carlos Hinojosa","Jorge Bacca","Henry Arguello"],"categories":null,"content":"","date":1540584915,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540584915,"objectID":"7c8965fbe519fd2661f0010aff5ad5bb","permalink":"http://carloshinojosa.me/publication/journal-coded-design-clustering2018/","publishdate":"2018-12-17T15:15:15-05:00","relpermalink":"/publication/journal-coded-design-clustering2018/","section":"publication","summary":"This paper proposes to design a set of coding patterns such that inter-class and intra-class data structure is preserved after the CSI acquisition in order to improve clustering results directly on the compressed domain. To validate the coding pattern design, an algorithm based on sparse subspace clustering (SSC) is proposed to perform clustering on the compressed measurements.","tags":["Compressive Spectral imaging","Coded Aperture Design","Unsupervised Classification","Subspace Clustering"],"title":"Coded aperture design for compressive spectral subspace clustering","type":"publication"},{"authors":null,"categories":null,"content":"Food and Agriculture Organization of the United Nations (FAO) has defined that agriculture is the main source of food supply given the high demand related to the population growth. Therefore, the FAO has also defined the trend to increase agricultural productivity based on the use of technology, information, and com- munication (ICT) tools. Nowadays, recent ICT tools like smartphones, cloud computing, Internet of Things (IoT), and big data support the implementation of precision agriculture to improve crops and their management. Colombia is an abundant country with suitable soils for the cultivation of diverse types of crops. However, the unknown ICT advantages for the farmers and the user rejection to adopt new technology difficult ICT implementation in the Colombian agriculture. On the other hand, the use of smartphones in various industrial applications has brought several advantages, due to their versatility, low-cost, and ease of use. Therefore, smart- phones can be used to address agriculture issues with a promissory implementation. In this sense, this work presents a smartphone application, called AgroTIC, for implementing ICT tools in the traditional work of the Colombian agricultural sector in order to support the productivity of Colombian farmers. AgroTIC takes advantage of smartphone sensors and several ICT tools. Specifically, AgroTIC is composed of four main modules: I) Communication module, II) Image processing and estimation of visible vegetation indices module, III) Production module, and IV) Marketing module. The first allows communication between farmer-to-farmer, and farmers-to-specialists/agronomist for technical assistance remote. The second module is a tool that provides an RGB analysis based on the estimation of two visible vegetation indices to help agronomist in diagnosis. The third module allows farmers to enter information related to their crops to estimate volumes of production and prepare the information for the marketing process. In the fourth module, the farmers can offer their products and establish a direct contact with different potential buyers. AgroTIC has been developed for a community of farmers who grow citrus fruits (orange, tangerine, and Tahiti lime) in the municipality of Simacota, Santander, Colombia.\nThis is the Model-View-ViewModel architectural pattern of the AgroTIC App\n","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"96b79765ef03b7db903d835d44d94188","permalink":"http://carloshinojosa.me/project/agrotic-mobile-app/","publishdate":"2018-06-01T00:00:00Z","relpermalink":"/project/agrotic-mobile-app/","section":"project","summary":"In this project we developed AgroTIC, a smartphone-based application for the agricultural sector in Colombia aimed at reducing such digital breach through smartphones low-cost and ubiquitous technology.","tags":["Remote Technical Assistance","Scientific Mobile App","Mobile App","Software"],"title":"Smartphone-based application for agricultural remote technical assistance in Colombia","type":"project"},{"authors":["Claudia V. Correa","Carlos Hinojosa","Henry Arguello"],"categories":null,"content":"","date":1479845769,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1479845769,"objectID":"a692b6217439c7a86fb61b481f9352c7","permalink":"http://carloshinojosa.me/publication/journal-sccsi-multishot2017/","publishdate":"2016-12-21T15:16:09-05:00","relpermalink":"/publication/journal-sccsi-multishot2017/","section":"publication","summary":"This paper extends the concept of SCCSI to a system admitting multiple snapshot acquisition by rotating the dispersive element, so the dispersed spatio-spectral source is coded and integrated at different detector pixels in each rotation. Thus, a different set of coded projections is captured using the same optical components of the original architecture.","tags":["Compressive Spectral imaging","Compressive Sensing","Color Coded Aperture"],"title":"Multiple snapshot colored compressive spectral imager","type":"publication"},{"authors":null,"categories":null,"content":"Time-domain seismic imaging, also known as time migration, is a fast and robust process for areas with mild lateral velocity variation. Even so, structural problems by mild lateral velocity variation can appear under this approach. An alternative known as depth migration manages to mitigate this problem in areas with lateral velocity variation. Therefore, time-to-depth conversion techniques have been employed to obtain seismic interval velocity. In detail, these techniques consist of solving a system of partial differential equations ill-posed with missing boundary conditions, which could lead to stability issues. Thus, an alternative two-step approach, solution of a system of partial differential equations, and interval velocity estimation from an optimization problem were proposed to take into account lateral velocity variations for the 2D case. However, the formulation of an optimization problem for the 3D case is not direct, and it is crucial for the seismic analysis in the interior of the earth.\nIn this project, we develop optimization problems to estimate the seismic interval velocity from Dix velocity on 2D and 3D data using image ray theory. Furthermore, we develop different modules for the DecisionSpace Geoscience software to implement the designed algorithms.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"f7411501d5c814e7d78b0afd13aa0fba","permalink":"http://carloshinojosa.me/project/seismic-time-to-depth/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/seismic-time-to-depth/","section":"project","summary":"Research project developed with Ecopetrol S.A., which includes the development of optimization algorithms for time to depth conversion, and development of modules for DesicionSpace Geoscience Software.","tags":["Seismic","Time-to-depth conversion","DecisionSpace Geoscience","Software"],"title":"Time to depth conversion and seismic interval velocity estimation","type":"project"},{"authors":["Carlos Hinojosa"],"categories":[],"content":"Create a free website with academia using Markdown, Jupyter, or RStudio. Choose a beautiful color theme and build anything with the Page Builder - over 40 widgets, themes, and language packs included!\nCheck out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nSetup academia Get Started View the documentation Ask a question Request a feature or report a bug Updating? View the Update Guide and Release Notes Support development of academia: Donate a coffee Become a backer on Patreon Decorate your laptop or journal with an academia sticker Wear the T-shirt Key features:\nPage builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 15+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files. Color Themes academia comes with day (light) and night (dark) mode built-in. Click the sun/moon icon in the top right of the Demo to see it in action!\nChoose a stunning color and font theme for your site. Themes are fully customizable and include:\nEcosystem academia Admin: An admin tool to import publications from BibTeX or import assets for an offline site academia Scripts: Scripts to help migrate content to new versions of academia Install You can choose from one of the following four methods to install:\none-click install using your web browser (recommended) install on your computer using Git with the Command Prompt/Terminal app install on your computer by downloading the ZIP files install on your computer with RStudio Then personalize and deploy your new site.\nUpdating View the Update Guide.\nFeel free to star the project on Github to help keep track of updates.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"http://carloshinojosa.me/post/getting-started/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Create a beautifully simple website in under 10 minutes.","tags":["academia"],"title":"academia: the website builder for Hugo","type":"post"}]